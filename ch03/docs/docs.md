# Chapter 03. 신경망 
신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 갖추고있다는 것이 가장 중요한 점이다.

## 3.1 신경망의 구조
![신경망](../assets/ANN.png)

신경망은 여려층의 노드의 배열로 주로 **입력층, 은닉층, 출력층**으로 구분된다.

입력층은 아무 연산이 이루어지지 않으며 입력을 받아서 다음 계층으로 넘기는 역할을 한다.

은닉층은 복잡한 문제를 해결하여 학습하는 핵심계층이다.

출력층은 은닉층에서의 신호를 외부로 출력하는데 사용된다.

## 3.2 활성화 함수
활성화 함수는 입력 신호의 총합을 출력신호로 변환하는 함수를 의미한다.
이 함수의 결과에 따라 다음 퍼셉트론으로 신호를 보낼지에 대한 여부와 신호의 강도를 결정하게 하므로 활성화 함수는 신경망의 복잡도를 높이는 아주 중요한 요소이다.

### 3.2.1 시그모이드 함수 Sigmoid function

$h(x) = \frac{1} {1 + exp(-x)}$

다음 함수는 신경망에서 자주 활성화되는 함수이다.

```py
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

![sigmoid](../assets/sigmoid.png)

시그모이드 함수의 치역은 (0,1)이며 연속적인 값을 가진다.

### 3.2.2 계단 함수
계단함수는 입력값이 0을 넘으면 1을 출력하고 그 이외에는 0을 출력하는 함수이다.
``` py
def step_funtion(x):
    y = x > 0 
    return y.astype(int)
```

![step_function](../assets/step.png)

이때, y는 `bool` 배열을 가지고있게 된다.
계단 함수는 0을 경계로 출력이 0에서 1로 바뀌는 형태를 가지고있다.

### 3.2.5 시그모이드 함수와 계단 함수의 비교

![compare_step&sigmoid](../assets/compare.png)

치역은 [0,1]로 동일하나 '매끄러움'의 차이를 가진다.
`sigmoid`함수는 부드로운 곡선으로 입력에 따라 출력이 연속적으로 변화하는 반면 `step`함수는 0을 경계로 출력이 급격하게 바뀌게 된다.
`sigmoid`함수의 이런 연속적인 매끈함은 신경망 학습에 매우 중요하다.

### 3.2.7 ReLU 함수
`ReLU(Rectified Linear Unit)` 함수는 0을 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수이다.
``` py
def relu(x):
    return np.maximum(0,x)
```

![ReLU](../assets/ReLU.png)

$h(x) = 
\left\{\begin{matrix} x ( x >= 0 )
 \\ 0 ( x <= 0 )
\end{matrix}\right.$